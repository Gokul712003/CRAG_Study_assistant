{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatNVIDIA(\n",
    "  model=\"meta/llama-3.3-70b-instruct\",\n",
    "  api_key=\"DONT STEAL MY API GET YOUR OWN\", \n",
    "  temperature=0.2,\n",
    ")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text splitting function\n",
    "def split_text(texts):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    doc_chunks = text_splitter.create_documents(texts)\n",
    "    for i, doc in enumerate(doc_chunks):\n",
    "        doc.metadata = {\n",
    "            \"page_number\": i + 1\n",
    "        }\n",
    "    return doc_chunks\n",
    "\n",
    "# Context addition prompts\n",
    "prompt_document = PromptTemplate(\n",
    "    input_variables=[\"WHOLE_DOCUMENT\"], template=\"{WHOLE_DOCUMENT}\"\n",
    ")\n",
    "\n",
    "prompt_chunk = PromptTemplate(\n",
    "    input_variables=[\"CHUNK_CONTENT\"],\n",
    "    template=\"Here is the chunk we want to situate within the whole document\\n\\n{CHUNK_CONTENT}\\n\\n\"\n",
    "    \"Please give a short succinct context to situate this chunk within the overall document for \"\n",
    "    \"the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual chunks\n",
    "def create_contextual_chunks(chunks_, llm, whole_document):\n",
    "    contextual_documents = []\n",
    "    for chunk in tqdm.tqdm(chunks_):\n",
    "        context = prompt_document.format(WHOLE_DOCUMENT=whole_document)\n",
    "        chunk_context = prompt_chunk.format(CHUNK_CONTENT=chunk.page_content)\n",
    "        llm_response = llm.invoke(context + chunk_context)\n",
    "        page_content = f\"Text: {chunk.page_content}\\n\\nContext: {llm_response.content}\"\n",
    "        doc = Document(page_content=page_content, metadata=chunk.metadata)\n",
    "        contextual_documents.append(doc)\n",
    "    return contextual_documents\n",
    "\n",
    "# Initialize Google Generative AI embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Modify the create_vector_store function to support persistence\n",
    "# def create_vector_store(documents, embeddings, persist_dir=\"study_materials\"):\n",
    "#     # Create persist directory if it doesn't exist\n",
    "#     Path(persist_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     return Chroma.from_documents(\n",
    "#         documents, \n",
    "#         embeddings,\n",
    "#         persist_directory=persist_dir\n",
    "#     )\n",
    "\n",
    "# # Add function to process multiple PDFs\n",
    "# def process_pdfs_to_vectorstore():\n",
    "#     pdf_folder = \"SPARK\"\n",
    "#     persist_dir = \"study_materials\"\n",
    "    \n",
    "#     # Get list of PDFs\n",
    "#     pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "#     all_contextual_chunks = []\n",
    "    \n",
    "#     for pdf_file in pdf_files:\n",
    "#         print(f\"Processing {pdf_file}...\")\n",
    "#         pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        \n",
    "#         # Load PDF\n",
    "#         pdf_loader = PyPDFLoader(pdf_path)\n",
    "#         raw_documents = pdf_loader.load()\n",
    "#         raw_text = \" \".join([doc.page_content for doc in raw_documents])\n",
    "        \n",
    "#         # Split text\n",
    "#         chunks = split_text([raw_text])\n",
    "        \n",
    "#         # Add source document metadata\n",
    "#         for chunk in chunks:\n",
    "#             chunk.metadata[\"source\"] = pdf_file\n",
    "            \n",
    "#         # Create contextual chunks\n",
    "#         contextual_chunks = create_contextual_chunks(chunks, llm, raw_text)\n",
    "#         all_contextual_chunks.extend(contextual_chunks)\n",
    "#         print(f\"Added {len(contextual_chunks)} chunks from {pdf_file}\")\n",
    "    \n",
    "#     # Create and persist vector store with all documents\n",
    "#     vector_store = create_vector_store(all_contextual_chunks, embeddings)\n",
    "#     vector_store.persist()\n",
    "#     print(f\"Created persistent vector store with {len(all_contextual_chunks)} total chunks\")\n",
    "    \n",
    "#     return vector_store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Spark_Distributed_Data_Processing_Unit1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:07<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 77 chunks from Spark_Distributed_Data_Processing_Unit1.pdf\n",
      "Processing Spark_Distributed_Data_Processing_Unit2.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [02:06<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 73 chunks from Spark_Distributed_Data_Processing_Unit2.pdf\n",
      "Processing Spark_Distributed_Data_Processing_Unit3.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [02:15<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 74 chunks from Spark_Distributed_Data_Processing_Unit3.pdf\n",
      "Processing Spark_Distributed_Data_Processing_Unit4.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [03:21<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 102 chunks from Spark_Distributed_Data_Processing_Unit4.pdf\n",
      "Processing Spark_Distributed_Data_Processing_Unit5.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [05:23<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 136 chunks from Spark_Distributed_Data_Processing_Unit5.pdf\n",
      "Created persistent vector store with 462 total chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14932\\456329420.py:47: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# Use this to create the vector store initially\n",
    "vector_store = process_pdfs_to_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6124\\956549323.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Later, to load the existing vector store:\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"study_materials\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query across all PDFs\n",
    "retriever = vector_store.as_retriever()\n",
    "# You can now query across all PDFs\n",
    "results = retriever.invoke(\"What is the definition of a data lake?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_number': 89, 'source': 'Spark_Distributed_Data_Processing_Unit4.pdf'}, page_content='Text: Spark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nMemory Sink\\n146\\n Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nFile Sink\\n• A transactional storage layer that combines the features of data lakes and data\\nwarehouses, supporting ACID transactions on large datasets.\\n• Supports append, update, and complete.\\n• Best for applications needing reliable data storage with support for scalable and\\natomic writes, such as slowly changing dimensions (SCD) or high-quality data\\nlakes that are frequently queried.\\n147 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nDelta Lake Sink\\n148\\n Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\n\\nContext: The chunk is part of a larger document discussing Spark Distributed Data Processing, specifically focusing on Output Modes in Structured Streaming, where it describes the characteristics and use cases of different output sinks, including Memory Sink, File Sink, and Delta Lake Sink.'),\n",
       " Document(metadata={'page_number': 101, 'source': 'Spark_Distributed_Data_Processing_Unit4.pdf'}, page_content='Text: • b) Dataset\\n• c) Streaming DataFrame\\n• d) Resilient Distributed Streams\\n164 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nMCQ Brainstorm…\\n• What does Structured Streaming use to process data?\\n• a) Micro-batch processing\\n• b) Continuous real-time processing\\n• c) Both a and b4\\n• d) None of the above\\n165 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nMCQ Brainstorm…\\n• In Structured Streaming, what is a sink?\\n• a) A function for reading data from a source\\n• b) A query plan optimizer\\n• c) A target where the output of a query is written\\n• d) A configuration property\\n166 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nMCQ Brainstorm…\\n• What is the purpose of a watermark in Structured Streaming?\\n• a) To track progress in the input data\\n• b) To handle late-arriving data\\n\\nContext: The chunk is part of a larger document discussing Spark Distributed Data Processing, specifically focusing on Structured Streaming, and is situated within a section of multiple-choice questions (MCQs) that test understanding of key concepts in Spark Structured Streaming.'),\n",
       " Document(metadata={'page_number': 74, 'source': 'Spark_Distributed_Data_Processing_Unit3.pdf'}, page_content='Text: • a) CSV\\n• b) JSON\\n• c) ORC\\n• d) Parquet\\n168 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark-SQL Hands-On\\nSpark-SQL Hands-On\\nMCQ –Brainstorm…\\n• What is a DataFrame in SparkSQL?\\n• a) A distributed collection of key-value pairs\\n• b) A distributed collection of rows organized into named columns\\n• c) A framework for machine learning models\\n• d) A tool for graph processing\\n169 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark-SQL Hands-On\\nSensitivity: L&T EduTech and LTIMindtree Use only\\n170 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark-SQL Hands-On\\nThank You !!!\\n171\\n\\nContext: The chunk is part of a Spark SQL hands-on training document, specifically within the section of multiple-choice questions (MCQs) for brainstorming, focusing on Spark SQL concepts and features.'),\n",
       " Document(metadata={'page_number': 87, 'source': 'Spark_Distributed_Data_Processing_Unit4.pdf'}, page_content='Text: file (e.g., in Parquet or JSON format) and for creating data lakes or storage layers\\nfor later batch processing.\\n141 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nFile Sink\\n142\\n Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nKafka Sink\\n• Writes streaming data to a Kafka topic.\\n• Kafka is commonly used for real-time messaging and integration\\n• Supports append mode.\\n• Ideal for applications that need real-time event distribution, such as integrating\\nmultiple services through Kafka or creating data pipelines.\\n143 Sensitivity: L&T EduTech and LTIMindtree Use only\\nSpark Distributed Data Processing\\nSpark Distributed Data Processing\\nOutput Modes in Structured Streaming\\nOutput Modes and Sink\\nKafka Sink\\n144\\n\\nContext: The chunk is part of a discussion on Output Modes in Structured Streaming, specifically focusing on the different types of sinks available, such as File Sink and Kafka Sink, and their uses in Spark Distributed Data Processing.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_documents(new_pdf_paths):\n",
    "    # Load existing vector store\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=\"study_materials\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    for pdf_path in new_pdf_paths:\n",
    "        print(f\"Processing {pdf_path}...\")\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        raw_documents = pdf_loader.load()\n",
    "        raw_text = \" \".join([doc.page_content for doc in raw_documents])\n",
    "        \n",
    "        chunks = split_text([raw_text])\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "            \n",
    "        contextual_chunks = create_contextual_chunks(chunks, llm, raw_text)\n",
    "        \n",
    "        # Add new documents to existing store\n",
    "        vector_store.add_documents(contextual_chunks)\n",
    "    \n",
    "    # Persist the updated store\n",
    "    vector_store.persist()\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Security Essentials in Applied AI_Unit1.pdf',\n",
       " 'Security Essentials in Applied AI_Unit2.pdf',\n",
       " 'Security Essentials in Applied AI_Unit3.pdf',\n",
       " 'Security Essentials in Applied AI_Unit4.pdf',\n",
       " 'Security Essentials in Applied AI_Unit5.pdf',\n",
       " 'LLM_Large_Language_Models_Unit1.pdf',\n",
       " 'LLM_Large_Language_Models_Unit2.pdf',\n",
       " 'LLM_Large_Language_Models_Unit3.pdf',\n",
       " 'LLM_Large_Language_Models_Unit4.pdf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Security Essentials in Applied AI_Unit1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:58<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Security Essentials in Applied AI_Unit2.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:22<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Security Essentials in Applied AI_Unit3.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:26<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Security Essentials in Applied AI_Unit4.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:32<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Security Essentials in Applied AI_Unit5.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:49<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LLM_Large_Language_Models_Unit1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:37<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LLM_Large_Language_Models_Unit2.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:44<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LLM_Large_Language_Models_Unit3.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [01:03<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LLM_Large_Language_Models_Unit4.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:59<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Add new PDFs to existing store\n",
    "# new_pdf_paths = pdf_names\n",
    "# updated_store = add_new_documents(new_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in store: 1010\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents in store: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_store.similarity_search(query=\"GDPR\",k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_number': 2, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content=\"Text: Compliance in Data Privacy and Cybersecurity\\n \\nSecurity Essentials in Applied AI Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\n➢ Data privacy regulations are essential to protect individuals' personal information from \\nmisuse, unauthorized access, and breaches. \\n➢ They ensure organizations handle data responsibly and transparently, fostering trust \\nbetween consumers and businesses. \\n➢ Such regulations also mitigate risks of identity theft, fraud, and privacy violations, while \\npromoting accountability in data management.\\nData Privacy Regulations Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\nRegion: European Union (EU) and European Economic Area (EEA).\\nPurpose: GDPR aims to protect the personal data and privacy of EU/EEA citizens and\\n\\nContext: This chunk is part of a larger document discussing Security Essentials in Applied AI, specifically focusing on Compliance in Data Privacy and Cybersecurity, and introduces key provisions of major data privacy regulations such as GDPR.\"),\n",
       " Document(metadata={'page_number': 3, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: Image Source: techaheadcorp\\nRegion: European Union (EU) and European Economic Area (EEA).\\nPurpose: GDPR aims to protect the personal data and privacy of EU/EEA citizens and \\nresidents. It governs how businesses collect, store, and process personal data.\\nKey Features: Consent, Right to Access, Right to Erasure, Data Portability, Penalties\\nGDPR: General Data Protection Regulation Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\nRegion: California, USA\\nPurpose: CCPA is designed to give California residents greater control over their personal \\ninformation collected by businesses.\\nKey Features: Right to Know, Right to Delete, Right to Opt-Out, Non-Discrimination\\nCCPA: California Consumer Privacy Act Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\nKey data privacy laws\\n➢ GDPR\\n\\nContext: This chunk is part of a larger document discussing Security Essentials in Applied AI, specifically focusing on Compliance in Data Privacy and Cybersecurity, and outlines key provisions of major data privacy regulations such as GDPR and CCPA.'),\n",
       " Document(metadata={'page_number': 15, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: Security Essentials in Applied AI\\nGDPR Compliance Principles\\nImage Source:Scalefocus\\n➢ The General Data Protection Regulation (GDPR) is a data protection and privacy law in \\nthe European Union (EU).\\n➢ Its primary aim is to give individuals control over their personal data and to simplify the \\nregulatory environment for international business by unifying data protection laws \\nwithin the EU.\\nGDPR compliance principles and implementation strategies for AI Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nGDPR Compliance Principles\\nData Minimization\\nImage Source: cedcoss.com\\n➢ AI systems should only collect the data \\nnecessary for their specific purpose. \\n➢ Ensure that personal data is not \\nexcessively gathered or stored beyond the \\ntime required for processing. \\n➢ Implement strategies like data \\nanonymization or pseudonymization to \\nreduce privacy risks while maintaining the \\neffectiveness of AI models.\\n\\nContext: The chunk is part of a larger document discussing Security Essentials in Applied AI, specifically focusing on compliance with data privacy regulations such as GDPR, and how AI systems can be designed to meet these regulatory requirements.'),\n",
       " Document(metadata={'page_number': 14, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: •Ethical and Legal Risks\\n•Impact: Non-compliance with data privacy regulations can result in significant legal penalties, \\nreputational damage, and loss of consumer trust.\\n•Outcome: AI companies need to integrate legal and ethical considerations into their development \\nprocesses to avoid risks associated with misuse of personal data.\\n•Cost of Compliance\\n•Impact: Continuous monitoring, audits, and legal consultations are required to ensure ongoing \\ncompliance with privacy regulations, which can increase development and operational costs.\\n•Outcome: Budget allocation for compliance activities becomes a significant aspect of AI project \\nplanning and execution.\\nData Privacy Regulation Impact on Development and Deployment Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nGDPR Compliance Principles\\nImage Source:Scalefocus\\n➢ The General Data Protection Regulation (GDPR) is a data protection and privacy law in \\nthe European Union (EU).\\n\\nContext: The chunk is situated within a section discussing the impact of data privacy regulations on AI development, specifically focusing on GDPR compliance principles and their implementation in AI projects.'),\n",
       " Document(metadata={'page_number': 27, 'source': 'Security Essentials in Applied AI_Unit1.pdf'}, page_content='Text: ▪ Overview:\\n✓ Enacted by the European Union to regulate data privacy and protection.\\n✓ Applies to all entities processing data of EU citizens, regardless of location.\\n▪ Key Provisions:\\n1. Right to Explanation: Individuals can request clarification on AI-driven decisions.\\n2. Data Protection: Mandates robust security measures for data handling.\\n3. Consent Requirements: Ensures individuals have control over their data.\\n▪ Impact:\\n✓ Promotes accountability in data-driven AI systems.\\n✓ Sets a global benchmark for privacy standards.\\n Sensitivity: LNT Construction Internal Use\\nIntroduction to Responsible AI Principles\\nSecurity Essentials in Applied AI\\nLegal and Ethical Frameworks for AI Governance\\n• IEEE Global Initiative on Ethics of AI\\n✓ Overview\\n• A framework developed by the Institute of Electrical and Electronics Engineers \\n(IEEE) to guide the ethical design and deployment of AI systems.\\n✓ Key Principles\\n• Human Rights\\n• Well-being\\n• Transparency\\n• Accountability\\n✓ Impact\\n\\nContext: The chunk discusses legal and ethical frameworks for AI governance, specifically focusing on the GDPR and IEEE Global Initiative on Ethics of AI, highlighting their key provisions, principles, and impact on promoting accountability and setting global benchmarks for privacy standards in AI systems.'),\n",
       " Document(metadata={'page_number': 4, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: Security Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\nKey data privacy laws\\n➢ GDPR\\nGeneral Data Protection \\nRegulation\\n➢ CCPA\\nCalifornia Consumer \\nPrivacy Act\\n Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nCompliance requirements for AI projects under GDPR \\n1.Data Protection by Design and by Default - Integrate privacy into AI design, using \\nminimal, pseudonymized, and encrypted data.\\n2.Lawful Basis for Processing - Ensure valid legal basis for data processing (e.g., consent, \\ncontract necessity).\\n3.Transparency and Informing Data Subjects - Provide clear privacy notices on data use \\nand AI processes.\\n4.Rights of Data Subjects - Allow access, rectification, erasure, and objection rights for \\nindividuals.\\n5.Data Minimization - Collect only necessary personal data for AI purposes. Sensitivity: LNT Construction Internal Use\\n\\nContext: This chunk is part of a larger document discussing Security Essentials in Applied AI, specifically focusing on compliance requirements for AI projects under major data privacy regulations such as GDPR and CCPA.'),\n",
       " Document(metadata={'page_number': 8, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: Security Essentials in Applied AI\\nKey Provisions of Major Data Privacy Regulations\\nImage Source: techaheadcorp\\n➢ Transparency in data collection, processing is key in implementing AI systems. \\n➢ Under GDPR and CCPA, organizations must provide clear notice about how personal \\ndata is being collected, processed, and used, and ensure that consent is obtained when \\nnecessary. \\n➢ AI systems should only collect and process the minimum amount of personal data \\nrequired for specific, legitimate purposes. \\n➢ Security measures that must be implemented to protect personal data in AI systems, \\nsuch as encryption and regular risk assessments. \\nCompliance requirements for AI Projects  Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nImpact of Data Privacy Regulations on AI Development\\nImage Source: pideeco.be\\n➢ Data privacy regulations, such as the GDPR in the EU or CCPA in California, set strict rules \\non how personal data is collected, stored, and used.\\n\\nContext: This chunk is part of a larger document discussing Security Essentials in Applied AI, specifically focusing on the impact of data privacy regulations such as GDPR and CCPA on AI development and compliance requirements for AI projects.'),\n",
       " Document(metadata={'page_number': 9, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: Image Source: pideeco.be\\n➢ Data privacy regulations, such as the GDPR in the EU or CCPA in California, set strict rules \\non how personal data is collected, stored, and used.\\n➢ These laws require AI systems to ensure data protection and security, limiting the types of \\ndata that can be used for training AI models\\n➢ Non-compliance can result in significant fines.\\nCompliance with Data Protection Laws Shapes AI Data Handling Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nImpact of Data Privacy Regulations on AI Development\\nImage Source:Matomo analytics\\n➢ Since many AI systems rely on vast amounts of personal data, privacy regulations limit \\naccess to sensitive information unless explicit consent is given.\\nInfluence on Data Availability and Quality for AI Training Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nImpact of Data Privacy Regulations on AI Development\\nImage Source: markovml.com\\n➢ AI developers must\\n\\nContext: The chunk discusses the impact of data privacy regulations, such as GDPR and CCPA, on AI development, including limitations on data collection and usage, and the need for explicit consent.'),\n",
       " Document(metadata={'page_number': 16, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content=\"Text: time required for processing. \\n➢ Implement strategies like data \\nanonymization or pseudonymization to \\nreduce privacy risks while maintaining the \\neffectiveness of AI models.\\nGDPR compliance principles and implementation strategies for AI Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nImage Source: cedcoss.com\\n➢ AI projects must ensure transparency in \\nhow personal data is collected, processed, \\nand used.\\n➢ Inform users about their data rights, the \\nAI's decision-making processes, and how \\ntheir data is being processed\\nGDPR Compliance Principles\\nTransparency and Informed Consent\\nGDPR compliance principles and implementation strategies for AI Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nImpact of Data Privacy Regulations on AI Development\\nImplementation Strategy Description\\nPrivacy by Design and by \\nDefault\\nIncorporate privacy features (data minimization, secure processing) from the start \\nof AI development.\\n\\nContext: The chunk is situated within a section of the document discussing GDPR compliance principles and implementation strategies for AI, specifically focusing on data minimization, transparency, and informed consent.\"),\n",
       " Document(metadata={'page_number': 22, 'source': 'Security Essentials in Applied AI_Unit4.pdf'}, page_content='Text: development and deployment\\n➢ AI systems can demonstrate GDPR compliance through regular audits, data access \\nlogs, and impact assessments\\nGDPR-compliant data processing and handling in AI applications Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nGDPR-Compliant Data Processing\\nImage Source: Sprinto\\n➢ AI models can be designed \\nto minimize the amount of \\npersonal data they process\\n➢ Design AI applications that \\nallow users to easily give, \\nwithdraw, and understand \\ntheir consent\\nGDPR-compliant data processing and handling in AI applications Sensitivity: LNT Construction Internal Use\\nSecurity Essentials in Applied AI\\nCompliance Requirements in Specific Industries\\nImage Source: GDPR local\\n➢ In healthcare, AI projects must comply with strict data privacy laws, such as the Health \\nInsurance Portability and Accountability Act (HIPAA)\\n➢ HIPAA’s Security Rule mandates that healthcare organizations and their business\\n\\nContext: The chunk is situated within the \"Security Essentials in Applied AI\" document, specifically in the section discussing GDPR compliance and data privacy regulations in various industries, including healthcare and finance.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDFs: ['Artificial_Intelligence_DL_GenAI_Unit1.pdf', 'Artificial_Intelligence_DL_GenAI_Unit2.pdf', 'Artificial_Intelligence_DL_GenAI_Unit3.pdf', 'Artificial_Intelligence_DL_GenAI_Unit4.pdf', 'Artificial_Intelligence_DL_GenAI_Unit5.pdf']\n"
     ]
    }
   ],
   "source": [
    "pdf_folders = [\"DLGAI\"]\n",
    "pdf_files = []\n",
    "\n",
    "for folder in pdf_folders:\n",
    "    folder_path = os.path.join(os.getcwd(), folder)\n",
    "    # Get only file names without folder paths\n",
    "    folder_pdfs = [pdf for pdf in os.listdir(folder_path) if pdf.endswith('.pdf')]\n",
    "    pdf_files.extend(folder_pdfs)\n",
    "\n",
    "print(f\"Found PDFs: {pdf_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artificial_Intelligence_DL_GenAI_Unit1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [04:50<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artificial_Intelligence_DL_GenAI_Unit2.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [03:53<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artificial_Intelligence_DL_GenAI_Unit3.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [04:17<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artificial_Intelligence_DL_GenAI_Unit4.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:33<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artificial_Intelligence_DL_GenAI_Unit5.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:33<00:00,  1.58s/it]\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6124\\3445932779.py:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# Add new PDFs to existing store\n",
    "new_pdf_paths = pdf_files\n",
    "updated_store = add_new_documents(new_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in store: 1427\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents in store: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
